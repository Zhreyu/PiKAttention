# PiKAttention  ğŸ±

*A tiny collection of CUDA kernels that showcase persistentâ€“kernel (PK) tricks for Transformer attention!*  

---

## âœ¨ Highlights

| Kernel | Idea in one line | .so produced |
|--------|-----------------|--------------|
| **Scalar** | naÃ¯ve FP16 MAC loop â€“ reference timing | `pk_attn_scalar.so` |
| **Opt** | microâ€‘unrolled & vectorised (float2/float4) | `pk_attn_opt.so` |
| **MMA** | WMMA Tensorâ€‘Core mma.sync (16Ã—16Ã—16) | `pk_attn_mma.so` |
| **DP4A** | 8â€‘bit INT4 KV, DP4A accumulate to FP16 | `pk_attn_dp4a.so` |
| **PiKAâ€‘BSR** | **P**ersistentâ€‘**K**ernel, blockâ€‘**A**ttention, 16â€‘token tiles kept in shared mem + userâ€‘supplied blockâ€‘sparse schedule | `pk_attn_bsr.so` |

### What â€œPiKAttentionâ€ means

> **PiKA** = **P**ersistentâ€‘**K**ernel **A**ttention.  
> One persistent CTA keeps a <code>16Â Ã—Â D</code> query tile in shared memory, streams just the K/V tiles it needs, and writes out the softmaxâ€‘weighted values â€“ cutting DRAM traffic to the bone.

I donâ€™t claim stateâ€‘ofâ€‘theâ€‘art â€“ the goal is to **demo clean PK patterns** you can graft into your own models.

---

## ğŸš€ QuickÂ start

```bash
# clone & build
make          # produces all *.so under default sm_86; override CUDA_ARCH if needed
!pip install -r requirements.txt
# reference benchmark (dense, 1Â kÂ tokens, 8Â heads, 128â€‘d)
python bench.py               

# sparse run: 2Â 048 tokens, 256â€‘d heads, 60â€¯% sparsity
python bench.py --seq 2048 --hdim 256 --sparsity 0.6

# full sweep with CSV + plots
python benchmark.py \
       --seq 1024 2048 \
       --heads 8 \
       --hdim 128 256 \
       --sparsity 0 0.5 0.9 \
       --loops 50 \
       --csv sweeps/pk_results.csv --plot
```

> ğŸ—’ï¸  **Flashâ€‘Attentionâ€‘2** timings are autoâ€‘included when the Python package is importable â€“ handy sanity check.

---

## ğŸ“‚ Repo layout

```
â”‚  bench.py            # quick singleâ€‘point benchmark
â”‚  benchmark.py        # multiâ€‘dim sweep + plots/CSV
â”‚  make_bsr.py         # make random blockâ€‘sparse schedules (.npy)
â”‚  Makefile            # toggles HEAD_DIM via env var, outputs *.so
â”‚
â”œâ”€ kernels/
â”‚   â”œâ”€ pk_attn_common.h  # cp.async helpers, rotary, etc.
â”‚   â”œâ”€ pk_attn_scalar.cu
â”‚   â”œâ”€ pk_attn_opt.cu
â”‚   â”œâ”€ pk_attn_mma.cu
â”‚   â”œâ”€ pk_attn_dp4a.cu
â”‚   â””â”€ pk_attn_bsr.cu    # the PiKA magic
â””â”€ sweeps/               # CSV & logs land here
```

---

## ğŸ“ˆ Example numbers (RTX3060Â â†¯Â sm86, FP16)

<!-- | seq | heads | dim | sparsity | Scalar | PiKAâ€‘BSR | speedâ€‘up |
|-----|-------|-----|----------|--------|----------|-----------|
|1â€¯024|Â 8|128|0.50|0.028Â Âµs|**0.004Â Âµs**| Ã—7.0|
|4â€¯096|16|256|0.60|0.012Â Âµs|**0.001Â Âµs**| Ã—12Â | -->

Latencies are per *tokenÂ·head*; lower is better.

---

## ğŸ§© Tinkering guide

* **Change head dim** â€“ reâ€‘export when building:
  ```bash
  HEAD_DIM=256 make clean all
  ```
* **Bring your own schedule** â€“ `make_bsr.py` can emit `.npy` that `bench.py --sched my.npy` consumes.
* **INT4 path** â€“ check `pk_attn_dp4a.cu`; INT4 quant + DP4A accumulate, handy for lowâ€‘mem.

---

## ğŸ”­ FutureÂ ideas

* columnâ€‘parallel PiKAâ€‘BSR for multiâ€‘SM scaling (1 block per head rightÂ now)
* FP8 kernels once H100 becomes dimeâ€‘aâ€‘dozen ğŸ¤“
* integrate Triton for autoâ€‘tuning (but still keep the raw CUDA around for learning)
* Paged KV Cache for decoding
* Embed in a model (YES)
---

## ğŸ“œ License

MIT â€“ do whatever, just keep a reference back.

